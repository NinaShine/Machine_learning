{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33b89fe-c26a-44e4-8cf4-e7acf0d7e41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/nina/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/nina/.local/lib/python3.10/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/nina/.local/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/nina/.local/lib/python3.10/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/nina/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/nina/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/nina/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/nina/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/nina/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nina/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nina/.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/nina/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in /home/nina/.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/nina/.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in /home/nina/.local/lib/python3.10/site-packages (from nltk) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f34215-6559-425b-9d87-329b4ff9c369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>316669998137483264</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>319090866545385472</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>322030931022065664</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>322694830620807168</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>328524426658328576</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id  \\\n",
       "0           0  316669998137483264   \n",
       "1           1  319090866545385472   \n",
       "2           2  322030931022065664   \n",
       "3           3  322694830620807168   \n",
       "4           4  328524426658328576   \n",
       "\n",
       "                                                text  science_related  \\\n",
       "0  Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1          McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2  Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3  Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4  Does daily routine help prevent problems with ...                1   \n",
       "\n",
       "   scientific_claim  scientific_reference  scientific_context  \n",
       "0               0.0                   0.0                 0.0  \n",
       "1               0.0                   0.0                 0.0  \n",
       "2               1.0                   0.0                 0.0  \n",
       "3               1.0                   0.0                 0.0  \n",
       "4               1.0                   0.0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du dataframe : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1140, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Six premi√®res lignes du dataframe : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>316669998137483264</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>319090866545385472</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>322030931022065664</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>322694830620807168</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>328524426658328576</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>331396203700944896</td>\n",
       "      <td>The Impact of Infertility on You and Your Rela...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id  \\\n",
       "0           0  316669998137483264   \n",
       "1           1  319090866545385472   \n",
       "2           2  322030931022065664   \n",
       "3           3  322694830620807168   \n",
       "4           4  328524426658328576   \n",
       "5           6  331396203700944896   \n",
       "\n",
       "                                                text  science_related  \\\n",
       "0  Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1          McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2  Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3  Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4  Does daily routine help prevent problems with ...                1   \n",
       "5  The Impact of Infertility on You and Your Rela...                0   \n",
       "\n",
       "   scientific_claim  scientific_reference  scientific_context  \n",
       "0               0.0                   0.0                 0.0  \n",
       "1               0.0                   0.0                 0.0  \n",
       "2               1.0                   0.0                 0.0  \n",
       "3               1.0                   0.0                 0.0  \n",
       "4               1.0                   0.0                 0.0  \n",
       "5               0.0                   0.0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Trois derni√®res lignes du dataframe : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1258</td>\n",
       "      <td>1341155832793165825</td>\n",
       "      <td>Whats the uber support team email address?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>1259</td>\n",
       "      <td>1344167355648241664</td>\n",
       "      <td>House passes bill to increase stimulus checks ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1260</td>\n",
       "      <td>1344485313222041600</td>\n",
       "      <td>@berriemoomin #Îü∞Ï•îÏùÑ_Í≥µÌèâÌïòÍ≤å_ÎåÄÌïòÏÑ∏Ïöî Renjun deserve be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0             tweet_id  \\\n",
       "1137        1258  1341155832793165825   \n",
       "1138        1259  1344167355648241664   \n",
       "1139        1260  1344485313222041600   \n",
       "\n",
       "                                                   text  science_related  \\\n",
       "1137         Whats the uber support team email address?                0   \n",
       "1138  House passes bill to increase stimulus checks ...                0   \n",
       "1139  @berriemoomin #Îü∞Ï•îÏùÑ_Í≥µÌèâÌïòÍ≤å_ÎåÄÌïòÏÑ∏Ïöî Renjun deserve be...                0   \n",
       "\n",
       "      scientific_claim  scientific_reference  scientific_context  \n",
       "1137               0.0                   0.0                 0.0  \n",
       "1138               0.0                   0.0                 0.0  \n",
       "1139               0.0                   0.0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cinq lignes au hasard du dataframe : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>339453990179844097</td>\n",
       "      <td>@bpkelly12 if that damn book lady decides to s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>1070</td>\n",
       "      <td>1204291217917435904</td>\n",
       "      <td>After Christmas and Boxing day All Roads lead ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>747</td>\n",
       "      <td>940635650746978305</td>\n",
       "      <td>The Arctic is getting warmer twice as fast as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>1026</td>\n",
       "      <td>1169834060039872512</td>\n",
       "      <td>A lavender bath cause a bitch feels like shit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>196</td>\n",
       "      <td>518499273285959680</td>\n",
       "      <td>11 Real-Life Dolls That Are Creepier Than ‚ÄòAnn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0             tweet_id  \\\n",
       "11           13   339453990179844097   \n",
       "972        1070  1204291217917435904   \n",
       "675         747   940635650746978305   \n",
       "931        1026  1169834060039872512   \n",
       "179         196   518499273285959680   \n",
       "\n",
       "                                                  text  science_related  \\\n",
       "11   @bpkelly12 if that damn book lady decides to s...                0   \n",
       "972  After Christmas and Boxing day All Roads lead ...                0   \n",
       "675  The Arctic is getting warmer twice as fast as ...                1   \n",
       "931      A lavender bath cause a bitch feels like shit                0   \n",
       "179  11 Real-Life Dolls That Are Creepier Than ‚ÄòAnn...                0   \n",
       "\n",
       "     scientific_claim  scientific_reference  scientific_context  \n",
       "11                0.0                   0.0                 0.0  \n",
       "972               0.0                   0.0                 0.0  \n",
       "675               1.0                   1.0                 1.0  \n",
       "931               0.0                   0.0                 0.0  \n",
       "179               0.0                   0.0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Nombre de tweets scientifiques : 375 '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Nombre de tweets non scientifiques : 765'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tweets CLAIM : 263\n",
      "Nombre de tweets REFERENCE : 203\n",
      "Nombre de tweets CONTEXT : 251\n",
      "Nombre de tweets CLAIM & REF : 124\n",
      "Nombre de tweets CLAIM & CONTEXT : 139\n",
      "Nombre de tweets CONTEXT & REF : 203\n",
      "Nombre total de tweets ayant au moins une cat√©gorie : 375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#attention le s√©parateur est une tabulation\n",
    "df=pd.read_csv('scitweets_export.tsv', sep='\\t')\n",
    "display (df.head())\n",
    "\n",
    "print (\"taille du dataframe : \\n\")\n",
    "display(df.shape)\n",
    "print (\"Six premi√®res lignes du dataframe : \\n\")\n",
    "display(df.head(6))\n",
    "print (\"\\n Trois derni√®res lignes du dataframe : \\n\")\n",
    "display(df.tail(3))\n",
    "print (\"\\n Cinq lignes au hasard du dataframe : \\n\")\n",
    "display(df.sample(5))\n",
    "\n",
    "nb_SC = df[df[\"science_related\"] == 1][\"science_related\"].count()\n",
    "nb_non_SC = df[df[\"science_related\"] == 0][\"science_related\"].count()\n",
    "\n",
    "display(f\"Nombre de tweets scientifiques : {nb_SC} \")\n",
    "display(f\"Nombre de tweets non scientifiques : {nb_non_SC}\")\n",
    "\n",
    "sc_bool = df[df[\"science_related\"] == 1]\n",
    "\n",
    "nb_claim = sc_bool[sc_bool[\"scientific_claim\"] == 1.0][\"tweet_id\"].count()\n",
    "nb_ref = sc_bool[sc_bool[\"scientific_reference\"] == 1.0][\"tweet_id\"].count()\n",
    "nb_context = sc_bool[sc_bool[\"scientific_context\"] == 1.0][\"tweet_id\"].count()\n",
    "\n",
    "nb_claim_ref = sc_bool[(sc_bool[\"scientific_claim\"] == 1.0) & (sc_bool[\"scientific_reference\"] == 1.0)][\"tweet_id\"].count()\n",
    "nb_claim_context = sc_bool[(sc_bool[\"scientific_claim\"] == 1.0) & (sc_bool[\"scientific_context\"] == 1.0)][\"tweet_id\"].count()\n",
    "nb_context_ref = sc_bool[(sc_bool[\"scientific_context\"] == 1.0) & (sc_bool[\"scientific_reference\"] == 1.0)][\"tweet_id\"].count()\n",
    "\n",
    "nb_total = sc_bool[(sc_bool[\"scientific_claim\"] == 1.0) |\n",
    "                   (sc_bool[\"scientific_reference\"] == 1.0) |\n",
    "                   (sc_bool[\"scientific_context\"] == 1.0)][\"tweet_id\"].count()\n",
    "\n",
    "\n",
    "print(f\"Nombre de tweets CLAIM : {nb_claim}\")\n",
    "print(f\"Nombre de tweets REFERENCE : {nb_ref}\")\n",
    "print(f\"Nombre de tweets CONTEXT : {nb_context}\")\n",
    "print(f\"Nombre de tweets CLAIM & REF : {nb_claim_ref}\")\n",
    "print(f\"Nombre de tweets CLAIM & CONTEXT : {nb_claim_context}\")\n",
    "print(f\"Nombre de tweets CONTEXT & REF : {nb_context_ref}\")\n",
    "print(f\"Nombre total de tweets ayant au moins une cat√©gorie : {nb_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41cc5cc-bfeb-43a2-9cfd-48943f5c8bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/nina/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/nina/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/nina/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/nina/.local/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/nina/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c1500d9-9fa6-4161-aec8-c48db57ad5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nina/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/nina/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/nina/nltk_data...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/nina/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Appliquer le pr√©traitement sur la colonne 'text'\u001b[39;00m\n\u001b[1;32m     58\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 59\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Cr√©ation de la matrice TF-IDF sur le texte nettoy√©\u001b[39;00m\n\u001b[1;32m     62\u001b[0m X_text \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Appliquer le pr√©traitement sur la colonne 'text'\u001b[39;00m\n\u001b[1;32m     58\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 59\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Cr√©ation de la matrice TF-IDF sur le texte nettoy√©\u001b[39;00m\n\u001b[1;32m     62\u001b[0m X_text \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text, remove_stopwords, stem, lemmatize)\u001b[0m\n\u001b[1;32m     35\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms.,!?;:()‚Ç¨$¬£‚ô•Ô∏è‚ú®üåüüåàüî•üòÇü§£üòäüòéüòâüòçü§îüëçüëèüí°üí¨üì¢üìàüìâü§ñ]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Tokenisation\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Stopwords\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_stopwords:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/nina/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#test de TF-IDF pour reduiore la taille de la matrice\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Fonction de pr√©traitement modifi√©e pour garder les emojis\n",
    "def preprocess_text(text, remove_stopwords=True, stem=False, lemmatize=False):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer URL, mentions, hashtags\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remplacer les nombres par '0'\n",
    "    text = re.sub(r'\\d+', '0', text)\n",
    "\n",
    "    # Garder seulement lettres, ponctuation basique, espaces, et emojis\n",
    "    emoji_pattern = r'[\\U00010000-\\U0010ffff]'\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:()‚Ç¨$¬£‚ô•Ô∏è‚ú®üåüüåàüî•üòÇü§£üòäüòéüòâüòçü§îüëçüëèüí°üí¨üì¢üìàüìâü§ñ]\", '', text)\n",
    "\n",
    "    # Tokenisation\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Appliquer le pr√©traitement sur la colonne 'text'\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "df['text_clean'] = df['text'].apply(lambda x: preprocess_text(x, remove_stopwords=True, stem=False, lemmatize=False))\n",
    "\n",
    "# Cr√©ation de la matrice TF-IDF sur le texte nettoy√©\n",
    "X_text = df['text_clean']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# Ajout des features binaires (d√©j√† dans ton DataFrame)\n",
    "extra_features = df[['has_url', 'has_mention', 'has_hashtag', 'has_emoji']].astype(int)\n",
    "X_extra = scipy.sparse.csr_matrix(extra_features.values)\n",
    "\n",
    "# Concat√©nation\n",
    "X_final = scipy.sparse.hstack([X_tfidf, X_extra])\n",
    "\n",
    "# Optionnel : r√©cup√©rer les noms de colonnes TF-IDF\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Affichage\n",
    "print(\"Taille de la matrice TF-IDF :\", X_tfidf.shape)\n",
    "print(\"Taille de la matrice finale :\", X_final.shape)\n",
    "display(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93190b-7d13-42de-8d23-c060d51bf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# V√©rifier la pr√©sence de valeurs manquantes et les supprimer si n√©cessaire\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# S√©lectionner les colonnes n√©cessaires\n",
    "X_text = df['text']  # Les tweets\n",
    "y = df['science_related']  # Les labels (scientifique ou non)\n",
    "\n",
    "# Cr√©ation de la matrice TF-IDF\n",
    "vectorizer = TfidfVectorizer()  # Initialisation du vectorizer\n",
    "X_tfidf = vectorizer.fit_transform(X_text)  # Transformation des tweets en vecteurs TF-IDF\n",
    "\n",
    "# Afficher la forme de la matrice r√©sultante\n",
    "print(\"Taille de la matrice TF-IDF :\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784628e2-b605-4dd8-b1b8-4331ac7286c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la matrice TF-IDF en DataFrame\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Afficher un extrait de la matrice TF-IDF\n",
    "print(\"Aper√ßu de la matrice TF-IDF (5 premi√®res lignes) :\")\n",
    "display(tfidf_df.head())  # Utilisation de display() pour bien formater l'affichage dans Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b7b6b-f031-4556-8aed-de3292b68243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (80% entra√Ænement, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['science_related'], test_size=0.2, random_state=42, stratify=df['science_related'])\n",
    "#print(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e8fe9-02a2-4fc6-aa41-e894abb2420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modele Na√Øve Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy du mod√®le Na√Øve Bayes : {accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nRapport de classification :\\n\", classification_report(y_test, y_pred))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualiser la matrice de confusion\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"NON-SCI\", \"SCI\"], yticklabels=[\"NON-SCI\", \"SCI\"])\n",
    "plt.xlabel(\"Pr√©diction\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de Confusion - Na√Øve Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e603c7-a942-4c70-b575-4d58b36285e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modele SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "model_svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Entra√Æner le mod√®le SVM\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"Accuracy du mod√®le SVM : {accuracy_svm:.2f}\")\n",
    "\n",
    "print(\"\\nRapport de classification SVM :\\n\", classification_report(y_test, y_pred_svm))\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Visualiser la matrice de confusion\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"NON-SCI\", \"SCI\"], yticklabels=[\"NON-SCI\", \"SCI\"])\n",
    "plt.xlabel(\"Pr√©diction\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de Confusion - SVM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1db4d-fc6c-430c-bb95-29017af11460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes kfold:\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# (80% entra√Ænement, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['science_related'], test_size=0.2, random_state=42, stratify=df['science_related'])\n",
    "#print(X_test)\n",
    "# Nombre d'it√©ration \n",
    "k = 5\n",
    "\n",
    "# Initialiser le mod√®le\n",
    "model = MultinomialNB()\n",
    "\n",
    "# D√©finir la strat√©gie de k-fold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculer les scores avec validation crois√©e\n",
    "\n",
    "cv_scores = cross_val_score(model, X_test, y_test, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Obtenir les pr√©dictions pour pouvoir cr√©er la matrice de confusion\n",
    "y_pred_cv = cross_val_predict(model, X_test, y_test, cv=kf)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(f\"Scores pour chaque fold: {cv_scores}\")\n",
    "print(f\"Pr√©cision moyenne: {np.mean(cv_scores):.2f}\")\n",
    "print(f\"√âcart-type: {np.std(cv_scores):.2f}\")\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification:\\n\", classification_report(y_test, y_pred_cv))\n",
    "\n",
    "# Matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_cv)\n",
    "\n",
    "# Visualiser la matrice de confusion\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[\"NON-SCI\", \"SCI\"], yticklabels=[\"NON-SCI\", \"SCI\"])\n",
    "plt.xlabel(\"Pr√©diction\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de Confusion - Na√Øve Bayes avec k-fold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870a187-3a56-4e2b-a6f0-bd4114f1a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les biblioth√®ques n√©cessaires\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# D√©finition de la validation crois√©e K-Fold (k=5)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "### üìå Mod√®le SVM avec K-Fold ###\n",
    "model_svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Calculer l'accuracy pour chaque fold\n",
    "accuracies_svm = cross_val_score(model_svm, X_tfidf, df['science_related'], cv=kf, scoring='accuracy')\n",
    "y_pred_svm = cross_val_predict(model_svm, X_tfidf, df['science_related'], cv=kf)\n",
    "\n",
    "# Affichage des accuracies et statistiques\n",
    "print(\"\\nüìä Accuracy du mod√®le SVM pour chaque K-Fold :\", accuracies_svm)\n",
    "print(f\"üìà Moyenne Accuracy SVM : {accuracies_svm.mean():.4f}\")\n",
    "print(f\"üìâ √âcart-Type Accuracy SVM : {accuracies_svm.std():.4f}\")\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "print(\"\\nüîç Rapport de classification SVM avec K-Fold :\\n\")\n",
    "print(classification_report(df['science_related'], y_pred_svm, target_names=[\"SCI\", \"NON-SCI\"]))\n",
    "\n",
    "# Matrice de confusion SVM\n",
    "conf_matrix_svm = confusion_matrix(df['science_related'], y_pred_svm)\n",
    "\n",
    "# üìä Affichage de la matrice de confusion SVM\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"SCI\", \"NON-SCI\"], yticklabels=[\"SCI\", \"NON-SCI\"])\n",
    "plt.xlabel(\"Pr√©diction\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de Confusion - SVM (K-Fold)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### üìå Mod√®le Random Forest avec K-Fold ###\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Calculer l'accuracy pour chaque fold\n",
    "accuracies_rf = cross_val_score(model_rf, X_tfidf, df['science_related'], cv=kf, scoring='accuracy')\n",
    "y_pred_rf = cross_val_predict(model_rf, X_tfidf, df['science_related'], cv=kf)\n",
    "\n",
    "# Affichage des accuracies et statistiques\n",
    "print(\"\\nüìä Accuracy du mod√®le Random Forest pour chaque K-Fold :\", accuracies_rf)\n",
    "print(f\"üìà Moyenne Accuracy Random Forest : {accuracies_rf.mean():.4f}\")\n",
    "print(f\"üìâ √âcart-Type Accuracy Random Forest : {accuracies_rf.std():.4f}\")\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "print(\"\\nüîç Rapport de classification Random Forest avec K-Fold :\\n\")\n",
    "print(classification_report(df['science_related'], y_pred_rf, target_names=[\"SCI\", \"NON-SCI\"]))\n",
    "\n",
    "# Matrice de confusion Random Forest\n",
    "conf_matrix_rf = confusion_matrix(df['science_related'], y_pred_rf)\n",
    "\n",
    "# üìä Affichage de la matrice de confusion Random Forest\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Oranges\", xticklabels=[\"SCI\", \"NON-SCI\"], yticklabels=[\"SCI\", \"NON-SCI\"])\n",
    "plt.xlabel(\"Pr√©diction\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de Confusion - Random Forest (K-Fold)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### üìà Comparaison des performances ###\n",
    "# Calcul des m√©triques (precision, recall, f1-score)\n",
    "metrics_svm = precision_recall_fscore_support(df['science_related'], y_pred_svm, average='binary')\n",
    "metrics_rf = precision_recall_fscore_support(df['science_related'], y_pred_rf, average='binary')\n",
    "\n",
    "# Cr√©ation du graphique de comparaison\n",
    "labels = ['Precision', 'Recall', 'F1-Score']\n",
    "svm_scores = metrics_svm[:3]\n",
    "rf_scores = metrics_rf[:3]\n",
    "\n",
    "x = np.arange(len(labels))  # Position des labels\n",
    "width = 0.35  # Largeur des barres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "rects1 = ax.bar(x - width/2, svm_scores, width, label='SVM', color='blue')\n",
    "rects2 = ax.bar(x + width/2, rf_scores, width, label='Random Forest', color='orange')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison des performances SVM vs Random Forest')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c710a5-61ad-4a1b-84be-c985d3d9cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353c016-d47f-494e-a91b-d28f27c0597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Importer les biblioth√®ques n√©cessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# üìå Charger les donn√©es\n",
    "file_path = \"scitweets_export.tsv\"  # Modifier si n√©cessaire\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# V√©rifier et supprimer les valeurs manquantes\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# S√©lectionner les colonnes n√©cessaires\n",
    "X_text = df['text']  # Les tweets\n",
    "y = df['science_related']  # Labels\n",
    "\n",
    "# üìå Cr√©ation de la matrice TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# üìå D√©finition de la validation crois√©e K-Fold (k=5)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "### üìå Mod√®le XGBoost avec K-Fold ###\n",
    "model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, booster='gblinear')\n",
    "\n",
    "# Calculer l'accuracy pour chaque fold\n",
    "accuracies_xgb = cross_val_score(model_xgb, X_tfidf, y, cv=kf, scoring='accuracy')\n",
    "y_pred_xgb = cross_val_predict(model_xgb, X_tfidf, y, cv=kf)\n",
    "\n",
    "# Affichage des accuracies et statistiques\n",
    "print(\"\\nüìä Accuracy du mod√®le XGBoost pour chaque K-Fold :\", accuracies_xgb)\n",
    "print(f\"üìà Moyenne Accuracy XGBoost : {accuracies_xgb.mean():.4f}\")\n",
    "print(f\"üìâ √âcart-Type Accuracy XGBoost : {accuracies_xgb.std():.4f}\")\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "print(\"\\nüîç Rapport de classification XGBoost avec K-Fold :\\n\")\n",
    "print(classification_report(y, y_pred_xgb, target_names=[\"SCI\", \"NON-SCI\"]))\n",
    "\n",
    "# Matrice de confusion XGBoost\n",
    "conf_matrix_xgb = confusion_matrix(y, y_pred_xgb)\n",
    "\n",
    "# üìä Affichage de la matrice de confusion XGBoost\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix_xgb, annot=True, fmt=\"d\", cmap=\"Purples\", xticklabels=[\"SCI\", \"NON-SCI\"], yticklabels=[\"SCI\", \"NON-SCI\"])\n",
    "plt.xlabel(\"Pr√©diction\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"Matrice de Confusion - XGBoost (K-Fold)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### üìà Comparaison des performances avec SVM et Random Forest ###\n",
    "# üìå Mod√®le SVM\n",
    "model_svm = SVC(kernel='linear', random_state=42)\n",
    "accuracies_svm = cross_val_score(model_svm, X_tfidf, y, cv=kf, scoring='accuracy')\n",
    "y_pred_svm = cross_val_predict(model_svm, X_tfidf, y, cv=kf)\n",
    "metrics_svm = precision_recall_fscore_support(y, y_pred_svm, average='binary')\n",
    "\n",
    "# üìå Mod√®le Random Forest\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "accuracies_rf = cross_val_score(model_rf, X_tfidf, y, cv=kf, scoring='accuracy')\n",
    "y_pred_rf = cross_val_predict(model_rf, X_tfidf, y, cv=kf)\n",
    "metrics_rf = precision_recall_fscore_support(y, y_pred_rf, average='binary')\n",
    "\n",
    "# üìå Calcul des m√©triques pour XGBoost\n",
    "metrics_xgb = precision_recall_fscore_support(y, y_pred_xgb, average='binary')\n",
    "\n",
    "# üìå Cr√©ation du graphique de comparaison\n",
    "labels = ['Precision', 'Recall', 'F1-Score']\n",
    "svm_scores = metrics_svm[:3]\n",
    "rf_scores = metrics_rf[:3]\n",
    "xgb_scores = metrics_xgb[:3]\n",
    "\n",
    "x = np.arange(len(labels))  # Position des labels\n",
    "width = 0.25  # Largeur des barres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "rects1 = ax.bar(x - width, svm_scores, width, label='SVM', color='blue')\n",
    "rects2 = ax.bar(x, rf_scores, width, label='Random Forest', color='orange')\n",
    "rects3 = ax.bar(x + width, xgb_scores, width, label='XGBoost', color='purple')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison des performances SVM vs Random Forest vs XGBoost')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a7526-f214-4a37-a9a5-0342b7ce7545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
