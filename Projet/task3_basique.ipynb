{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Basic Multi-label Classification Approach (Improved)\n",
    "\n",
    "This notebook implements an improved version of the SVM classifier with better class balancing and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, precision_score,\n",
    "    recall_score, accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import joblib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing text...\n",
      "\n",
      "Label distribution and weights:\n",
      "\n",
      "scientific_claim:\n",
      "Class 0: 877 (76.9%)\n",
      "Class 1: 263 (23.1%)\n",
      "Weights: {0: 1.0, 1: 5.001901140684411}\n",
      "\n",
      "scientific_reference:\n",
      "Class 0: 937 (82.2%)\n",
      "Class 1: 203 (17.8%)\n",
      "Weights: {0: 1.0, 1: 6.9236453201970445}\n",
      "\n",
      "scientific_context:\n",
      "Class 0: 889 (78.0%)\n",
      "Class 1: 251 (22.0%)\n",
      "Weights: {0: 1.0, 1: 5.312749003984064}\n"
     ]
    }
   ],
   "source": [
    "def basic_preprocess(text):\n",
    "    \"\"\"Minimal text preprocessing to preserve information\"\"\"\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep URLs and hashtags\n",
    "    text = re.sub(r'[^\\w\\s#@/:.]+', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"scitweets_export.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Basic preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "df['processed_text'] = df['text'].apply(basic_preprocess)\n",
    "\n",
    "# Prepare labels\n",
    "label_cols = [\"scientific_claim\", \"scientific_reference\", \"scientific_context\"]\n",
    "Y = df[label_cols].fillna(0).astype(int)\n",
    "\n",
    "# Calculate class weights for each label\n",
    "class_weights = []\n",
    "for col in Y.columns:\n",
    "    counts = Counter(Y[col])\n",
    "    # More aggressive weight balancing\n",
    "    weight = {0: 1.0, 1: counts[0] / counts[1] * 1.5}  # Increase minority class weight\n",
    "    class_weights.append(weight)\n",
    "\n",
    "# Display dataset stats\n",
    "print(\"\\nLabel distribution and weights:\")\n",
    "for col, weights in zip(label_cols, class_weights):\n",
    "    counts = Y[col].value_counts()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"Class 0: {counts[0]} ({counts[0]/len(Y)*100:.1f}%)\")\n",
    "    print(f\"Class 1: {counts[1]} ({counts[1]/len(Y)*100:.1f}%)\")\n",
    "    print(f\"Weights: {weights}\")\n",
    "\n",
    "# Create stratification column for multi-label data\n",
    "strat_col = Y.apply(lambda x: ''.join(x.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation with Improved Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kfold(X, Y, strat_col, class_weights, n_splits=5):\n",
    "    \"\"\"Perform k-fold cross-validation with improved pipeline\"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision_micro': [],\n",
    "        'recall_micro': [],\n",
    "        'f1_micro': [],\n",
    "        'precision_macro': [],\n",
    "        'recall_macro': [],\n",
    "        'f1_macro': []\n",
    "    }\n",
    "    \n",
    "    # For plotting\n",
    "    fold_metrics_history = []\n",
    "    \n",
    "    # Create improved TF-IDF vectorizer\n",
    "    print(\"Vectorizing text...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 3),     # Unigrams, bigrams, and trigrams\n",
    "        min_df=3,               # Remove very rare words\n",
    "        max_df=0.95,            # Remove very common words\n",
    "        strip_accents='unicode', # Handle French accents\n",
    "        sublinear_tf=True,      # Apply sublinear scaling\n",
    "        use_idf=True,           # Use inverse document frequency\n",
    "        lowercase=True\n",
    "    )\n",
    "    X_tfidf = vectorizer.fit_transform(X)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_tfidf, strat_col), 1):\n",
    "        print(f\"\\nProcessing fold {fold}/{n_splits}...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train = X_tfidf[train_idx]\n",
    "        X_val = X_tfidf[val_idx]\n",
    "        Y_train = Y.iloc[train_idx]\n",
    "        Y_val = Y.iloc[val_idx]\n",
    "        \n",
    "        # Create separate classifiers for each label with appropriate class weights\n",
    "        classifiers = []\n",
    "        for weights in class_weights:\n",
    "            base_svc = LinearSVC(\n",
    "                random_state=42,\n",
    "                max_iter=2000,\n",
    "                class_weight=weights,\n",
    "                dual=False,\n",
    "                C=1.0\n",
    "            )\n",
    "            # Add probability calibration\n",
    "            clf = CalibratedClassifierCV(base_svc, cv=3)\n",
    "            classifiers.append(clf)\n",
    "        \n",
    "        # Create multi-output classifier\n",
    "        multi_clf = MultiOutputClassifier(classifiers, n_jobs=-1)\n",
    "        \n",
    "        # Train model\n",
    "        multi_clf.fit(X_train, Y_train)\n",
    "        \n",
    "        # Predict\n",
    "        Y_pred = multi_clf.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fold_metrics = {\n",
    "            'accuracy': accuracy_score(Y_val, Y_pred),\n",
    "            'precision_micro': precision_score(Y_val, Y_pred, average='micro'),\n",
    "            'recall_micro': recall_score(Y_val, Y_pred, average='micro'),\n",
    "            'f1_micro': f1_score(Y_val, Y_pred, average='micro'),\n",
    "            'precision_macro': precision_score(Y_val, Y_pred, average='macro'),\n",
    "            'recall_macro': recall_score(Y_val, Y_pred, average='macro'),\n",
    "            'f1_macro': f1_score(Y_val, Y_pred, average='macro')\n",
    "        }\n",
    "        \n",
    "        # Store metrics\n",
    "        for metric, value in fold_metrics.items():\n",
    "            metrics[metric].append(value)\n",
    "        fold_metrics_history.append(fold_metrics)\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"Accuracy: {fold_metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 (micro): {fold_metrics['f1_micro']:.4f}\")\n",
    "        print(f\"F1 (macro): {fold_metrics['f1_macro']:.4f}\")\n",
    "        \n",
    "        # Print detailed classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(Y_val, Y_pred, target_names=label_cols))\n",
    "        \n",
    "        # Print label distribution\n",
    "        print(\"\\nPredicted label distribution in this fold:\")\n",
    "        for i, col in enumerate(Y_train.columns):\n",
    "            val_dist = np.bincount(Y_pred[:, i]) / len(Y_pred)\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"Distribution: {dict(enumerate(val_dist))}\")\n",
    "    \n",
    "    # Print overall results\n",
    "    print(\"\\n=== Overall Results ===\")\n",
    "    for metric, scores in metrics.items():\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        print(f\"{metric}: {mean:.4f} Â± {std:.4f}\")\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    metrics_df = pd.DataFrame(fold_metrics_history)\n",
    "    \n",
    "    for i, metric in enumerate(['accuracy', 'f1_micro', 'f1_macro']):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.plot(range(1, n_splits + 1), metrics_df[metric], marker='o')\n",
    "        plt.title(f'{metric.capitalize()} across Folds')\n",
    "        plt.xlabel('Fold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics, metrics_df, vectorizer, multi_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting K-Fold Cross-Validation ===\n",
      "Vectorizing text...\n",
      "\n",
      "Processing fold 1/5...\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'estimator' parameter of MultiOutputClassifier must be an object implementing 'fit' and 'predict'. Got [CalibratedClassifierCV(cv=3,\n                       estimator=LinearSVC(class_weight={0: 1.0,\n                                                         1: 5.001901140684411},\n                                           dual=False, max_iter=2000,\n                                           random_state=42)), CalibratedClassifierCV(cv=3,\n                       estimator=LinearSVC(class_weight={0: 1.0,\n                                                         1: 6.9236453201970445},\n                                           dual=False, max_iter=2000,\n                                           random_state=42)), CalibratedClassifierCV(cv=3,\n                       estimator=LinearSVC(class_weight={0: 1.0,\n                                                         1: 5.312749003984064},\n                                           dual=False, max_iter=2000,\n                                           random_state=42))] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Starting K-Fold Cross-Validation ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m metrics, metrics_df, vectorizer, best_clf = \u001b[43mevaluate_kfold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrat_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Training Final Model on Full Dataset ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Vectorize full dataset\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mevaluate_kfold\u001b[39m\u001b[34m(X, Y, strat_col, class_weights, n_splits)\u001b[39m\n\u001b[32m     54\u001b[39m multi_clf = MultiOutputClassifier(classifiers, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mmulti_clf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     60\u001b[39m Y_pred = multi_clf.predict(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ML/Machine_learning/Projet/venv/lib/python3.12/site-packages/sklearn/multioutput.py:543\u001b[39m, in \u001b[36mMultiOutputClassifier.fit\u001b[39m\u001b[34m(self, X, Y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, **fit_params):\n\u001b[32m    518\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[32m    519\u001b[39m \n\u001b[32m    520\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m \u001b[33;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m     \u001b[38;5;28mself\u001b[39m.classes_ = [estimator.classes_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.estimators_]\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ML/Machine_learning/Projet/venv/lib/python3.12/site-packages/sklearn/base.py:1382\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1377\u001b[39m partial_fit_and_fitted = (\n\u001b[32m   1378\u001b[39m     fit_method.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mpartial_fit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[32m   1379\u001b[39m )\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ML/Machine_learning/Projet/venv/lib/python3.12/site-packages/sklearn/base.py:436\u001b[39m, in \u001b[36mBaseEstimator._validate_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    429\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[32m    430\u001b[39m \n\u001b[32m    431\u001b[39m \u001b[33;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m    accepted constraints.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ML/Machine_learning/Projet/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'estimator' parameter of MultiOutputClassifier must be an object implementing 'fit' and 'predict'. Got [CalibratedClassifierCV(cv=3,\n                       estimator=LinearSVC(class_weight={0: 1.0,\n                                                         1: 5.001901140684411},\n                                           dual=False, max_iter=2000,\n                                           random_state=42)), CalibratedClassifierCV(cv=3,\n                       estimator=LinearSVC(class_weight={0: 1.0,\n                                                         1: 6.9236453201970445},\n                                           dual=False, max_iter=2000,\n                                           random_state=42)), CalibratedClassifierCV(cv=3,\n                       estimator=LinearSVC(class_weight={0: 1.0,\n                                                         1: 5.312749003984064},\n                                           dual=False, max_iter=2000,\n                                           random_state=42))] instead."
     ]
    }
   ],
   "source": [
    "print(\"=== Starting K-Fold Cross-Validation ===\")\n",
    "metrics, metrics_df, vectorizer, best_clf = evaluate_kfold(\n",
    "    df['processed_text'], Y, strat_col, class_weights, n_splits=5\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training Final Model on Full Dataset ===\")\n",
    "# Vectorize full dataset\n",
    "X_full = vectorizer.transform(df['processed_text'])\n",
    "\n",
    "# Create final classifiers with appropriate weights\n",
    "final_classifiers = []\n",
    "for weights in class_weights:\n",
    "    base_svc = LinearSVC(\n",
    "        random_state=42,\n",
    "        max_iter=2000,\n",
    "        class_weight=weights,\n",
    "        dual=False,\n",
    "        C=1.0\n",
    "    )\n",
    "    clf = CalibratedClassifierCV(base_svc, cv=3)\n",
    "    final_classifiers.append(clf)\n",
    "\n",
    "# Create and train final multi-output classifier\n",
    "final_clf = MultiOutputClassifier(final_classifiers, n_jobs=-1)\n",
    "final_clf.fit(X_full, Y)\n",
    "\n",
    "# Save model artifacts\n",
    "print(\"\\nSaving model and preprocessing components...\")\n",
    "model_artifacts = {\n",
    "    'classifier': final_clf,\n",
    "    'vectorizer': vectorizer,\n",
    "    'class_weights': class_weights\n",
    "}\n",
    "joblib.dump(model_artifacts, \"svm_classifier_improved.joblib\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive results\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Micro vs Macro metrics\n",
    "plt.subplot(2, 1, 1)\n",
    "metric_groups = ['macro', 'micro']\n",
    "metrics_to_plot = ['precision', 'recall', 'f1']\n",
    "\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "for i, group in enumerate(metric_groups):\n",
    "    metric_values = [np.mean(metrics[f'{metric}_{group}']) for metric in metrics_to_plot]\n",
    "    metric_stds = [np.std(metrics[f'{metric}_{group}']) for metric in metrics_to_plot]\n",
    "    \n",
    "    bars = plt.bar(x + i*width, metric_values, width, yerr=metric_stds, capsize=5,\n",
    "                   label=f'{group.capitalize()} Average')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xticks(x + width/2, metrics_to_plot)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Metrics stability across folds\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.boxplot(data=metrics_df[['accuracy', 'f1_micro', 'f1_macro']])\n",
    "plt.title('Metrics Distribution Across Folds')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance info\n",
    "print(\"\\nFeature Information:\")\n",
    "print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "feature_weights = pd.DataFrame({\n",
    "    'feature': vectorizer.get_feature_names_out(),\n",
    "    'idf': vectorizer.idf_\n",
    "})\n",
    "display(feature_weights.nlargest(20, 'idf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
