{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1826a1fa-697c-41f2-a6c3-1fefc86b034c",
   "metadata": {},
   "source": [
    "# Task 2: CLAIM/REF vs CONTEXT Classification\n",
    "\n",
    "This notebook implements a machine learning pipeline for classifying scientific tweets as CLAIM/REF or CONTEXT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3263e3d-8f4e-4e90-9ebb-191b74c7d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, make_scorer\n",
    ")\n",
    "import optuna\n",
    "import emoji\n",
    "import re\n",
    "import scipy.sparse\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import joblib\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Cr√©ation d'un r√©pertoire local au cas o√π les chemins par d√©faut posent probl√®me\n",
    "nltk_data_dir = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# Ajout manuel au chemin\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# T√©l√©chargements des ressources n√©cessaires dans le bon r√©pertoire\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_dir)\n",
    "nltk.download(\"stopwords\", download_dir=nltk_data_dir)\n",
    "nltk.download(\"wordnet\", download_dir=nltk_data_dir)\n",
    "\n",
    "\n",
    "\n",
    "#Librairies d'affichage\n",
    "from PIL import Image  # Gestion et manipulation d'images\n",
    "import plotly.graph_objs as go  # Cr√©ation de graphiques personnalis√©s avec Plotly\n",
    "import plotly.offline as py  # Mode offline pour afficher les graphiques Plotly\n",
    "import plotly.express as px  # Visualisation simplifi√©e et rapide avec Plotly\n",
    "import plotly.io as pio # Permet de sauvegarder l'image\n",
    "\n",
    "#Librairies Scikit-learn\n",
    "from sklearn.manifold import TSNE  # R√©duction de dimensions avec T-SNE\n",
    "from sklearn.decomposition import PCA  # R√©duction de dimensions avec ACP\n",
    "\n",
    "#Librairies UMAP\n",
    "import umap.plot  # Visualisation des r√©sultats de r√©duction de dimensions avec UMAP\n",
    "from umap import UMAP  # R√©duction de dimensions avec UMAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe83709-a2e5-4920-a14c-81ebe715380a",
   "metadata": {},
   "source": [
    "## InitialPipeline \n",
    "\n",
    "Pipeline for data exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c670123e-197c-4f8d-8288-e744d9aec64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InitialDataLoader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.filepath, sep='\\t')\n",
    "        print(\"\\nüìä Aper√ßu du dataset :\")\n",
    "        display(self.df.head())\n",
    "        print(\"Nombre total de lignes :\", len(self.df))\n",
    "        return self.df\n",
    "\n",
    "    def filter_scientific_rows(self):\n",
    "        df_sci = self.df[self.df[\"science_related\"] == 1]\n",
    "        print(\"Nombre de lignes scientifiques :\", len(df_sci))\n",
    "        return df_sci\n",
    "\n",
    "    def visualize_distribution(self, df_sci):\n",
    "        df_context = df_sci[df_sci[\"scientific_context\"] == 1.0]\n",
    "        df_context_unic = df_context[(df_context[\"scientific_claim\"] == 0.0) & (df_context[\"scientific_reference\"] == 0.0)]\n",
    "        df_claim_ref = df_sci[(df_sci[\"scientific_claim\"] == 1.0) & (df_sci[\"scientific_reference\"] == 1.0)]\n",
    "        df_claim_ou_ref = df_sci[(df_sci[\"scientific_claim\"] == 1.0) | (df_sci[\"scientific_reference\"] == 1.0)]\n",
    "        df_claim_ou_ref_unic = df_claim_ou_ref[df_claim_ou_ref[\"scientific_context\"] == 0.0]\n",
    "        df_claim_ref_context = df_sci[(df_sci[\"scientific_claim\"] == 1.0) & (df_sci[\"scientific_reference\"] == 1.0) & (df_sci[\"scientific_context\"] == 1.0)]\n",
    "\n",
    "        print(\"\\nüìä R√©partition des donn√©es :\")\n",
    "        print(\"CLAIM et REF uniquement (sans context) :\", len(df_claim_ref[(df_claim_ref[\"scientific_context\"] == 0.0)]))\n",
    "        print(\"CLAIM et REF (avec context possible) :\", len(df_claim_ref))\n",
    "        print(\"CLAIM ou REF (avec context) :\", len(df_claim_ou_ref))\n",
    "        print(\"CLAIM ou REF (sans context) :\", len(df_claim_ou_ref_unic))\n",
    "        print(\"CONTEXT uniquement (sans claim ou ref) :\", len(df_context_unic))\n",
    "        print(\"CONTEXT (avec claim ou ref)  :\", len(df_context))\n",
    "        print(\"CLAIM et REF et CONTEXT :\", len(df_claim_ref_context))\n",
    "\n",
    "        counts = {\n",
    "            \"CLAIM ou REF (avec context possible) \": len(df_claim_ou_ref),\n",
    "            \"CLAIM ou REF (sans context) \": len(df_claim_ou_ref_unic),\n",
    "            \"CONTEXT uniquement (sans claim ou ref) :\": len(df_context_unic),\n",
    "            \"CONTEXT (avec claim ou ref)  :\": len(df_context),\n",
    "            \"CLAIM et REF et CONTEXT :\": len(df_claim_ref_context)\n",
    "        }\n",
    "\n",
    "        df_counts = pd.DataFrame(list(counts.items()), columns=[\"Cat√©gorie\", \"Nombre\"])\n",
    "\n",
    "        plt.figure(figsize=(11, 7))\n",
    "        bars = plt.bar(df_counts[\"Cat√©gorie\"], df_counts[\"Nombre\"], \n",
    "                        color=[\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B3\", \"pink\"])\n",
    "        plt.title(\"R√©partition des types d'assertions scientifiques\", fontsize=14)\n",
    "        plt.xlabel(\"Type d'assertion\", fontsize=12)\n",
    "        plt.ylabel(\"Nombre de tweets\", fontsize=12)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, yval + 5, int(yval), ha='center', va='bottom', fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_combination_pie(self):\n",
    "        df = self.df.copy()\n",
    "        df = df[~((df[\"scientific_claim\"] == 0.0) & (df[\"scientific_reference\"] == 0.0) & (df[\"scientific_context\"] == 0.0))]\n",
    "        combinations = df[[\"scientific_claim\", \"scientific_reference\", \"scientific_context\"]].value_counts()\n",
    "        combinations.index = combinations.index.map(lambda x: f\"Claim={int(x[0])}, Ref={int(x[1])}, Context={int(x[2])}\")\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        combinations.plot(kind='pie', autopct='%1.1f%%', startangle=90, ylabel='',\n",
    "                          title='R√©partition des combinaisons CLAIM / REF / CONTEXT', fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09944b55-c8e6-43c8-b12e-4ad8e4c77307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = InitialDataLoader(\"scitweets_export.tsv\")\n",
    "df = loader.load_data()\n",
    "df_sci = loader.filter_scientific_rows()\n",
    "loader.visualize_distribution(df_sci)\n",
    "loader.plot_combination_pie()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6155eb-ad09-4726-a5dc-e3887fec1604",
   "metadata": {},
   "source": [
    "## ModelPipeline Class Definition\n",
    "\n",
    "Pipeline for data processing and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd32279e-c600-4ff1-a293-035dac5a872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self, name=\"ModelPipeline\"):\n",
    "        self.name = name\n",
    "        self.models = {\n",
    "            \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "            \"Na√Øve Bayes\": MultinomialNB(),\n",
    "            \"Random Forest\": RandomForestClassifier(),\n",
    "            \"SVM\": SVC(),\n",
    "            \"SVM linear\": LinearSVC(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"AdaBoost\": AdaBoostClassifier(),\n",
    "            \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "            \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "            \"Neural Network - MLP\": MLPClassifier(max_iter=300)\n",
    "        }\n",
    "        self.scorers = {\n",
    "            \"Accuracy\": make_scorer(accuracy_score),\n",
    "            \"Precision\": make_scorer(precision_score),\n",
    "            \"Recall\": make_scorer(recall_score),\n",
    "            \"F1 Score\": make_scorer(f1_score)\n",
    "        }\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.vectorizer = None\n",
    "\n",
    "    def analyze_data(self, df):\n",
    "        df_sci = df[df[\"science_related\"] == 1]\n",
    "\n",
    "        df_context = df_sci[df_sci[\"scientific_context\"] == 1.0]\n",
    "        df_context_unic = df_context[(df_context[\"scientific_claim\"] == 0.0) & (df_context[\"scientific_reference\"] == 0.0)]\n",
    "        df_claim_ref = df_sci[(df_sci[\"scientific_claim\"] == 1.0) & (df_sci[\"scientific_reference\"] == 1.0)]\n",
    "        df_claim_ou_ref = df_sci[(df_sci[\"scientific_claim\"] == 1.0) | (df_sci[\"scientific_reference\"] == 1.0)]\n",
    "        df_claim_ou_ref_unic = df_claim_ou_ref[df_claim_ou_ref[\"scientific_context\"] == 0.0]\n",
    "        df_claim_ref_context = df_sci[\n",
    "            (df_sci[\"scientific_claim\"] == 1.0) & \n",
    "            (df_sci[\"scientific_reference\"] == 1.0) & \n",
    "            (df_sci[\"scientific_context\"] == 1.0)\n",
    "        ]\n",
    "\n",
    "        counts = {\n",
    "            \"CLAIM ou REF (avec context possible)\": len(df_claim_ou_ref),\n",
    "            \"CLAIM ou REF (sans context)\": len(df_claim_ou_ref_unic),\n",
    "            \"CONTEXT uniquement (sans claim ou ref)\": len(df_context_unic),\n",
    "            \"CONTEXT (avec claim ou ref)\": len(df_context),\n",
    "            \"CLAIM et REF et CONTEXT\": len(df_claim_ref_context)\n",
    "        }\n",
    "\n",
    "        df_counts = pd.DataFrame(list(counts.items()), columns=[\"Cat√©gorie\", \"Nombre\"])\n",
    "\n",
    "        plt.figure(figsize=(11, 7))\n",
    "        bars = plt.bar(df_counts[\"Cat√©gorie\"], df_counts[\"Nombre\"], color=[\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B3\", \"#E9967A\"])\n",
    "        plt.title(\"R√©partition des types d'assertions scientifiques\", fontsize=14)\n",
    "        plt.xlabel(\"Type d'assertion\", fontsize=12)\n",
    "        plt.ylabel(\"Nombre de tweets\", fontsize=12)\n",
    "        plt.xticks(rotation=15)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, yval + 5, int(yval), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return df_sci\n",
    "\n",
    "    def create_label(self, df_sci):\n",
    "        df_sci = df_sci.copy()\n",
    "        df_sci[\"label\"] = df_sci.apply(\n",
    "            lambda row: 0 if (row[\"scientific_context\"] == 1.0 and row[\"scientific_claim\"] == 0.0 and row[\"scientific_reference\"] == 0.0)\n",
    "            else 1,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        print(\"\\n‚úîÔ∏è √âquilibrage final :\")\n",
    "        print(df_sci[\"label\"].value_counts())\n",
    "        return df_sci\n",
    "\n",
    "    def clean_text_light(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+\", \"URL\", text)\n",
    "        text = re.sub(r\"@\\w+\", \"MENTION\", text)\n",
    "        text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "        text = re.sub(r\":[^:\\s]+:\", \"EMOJI\", text)\n",
    "        return text\n",
    "\n",
    "    def MyCleanText(self, X,\n",
    "                    lowercase=True,\n",
    "                    removestopwords=False,\n",
    "                    removedigit=True,\n",
    "                    getstemmer=False,\n",
    "                    getlemmatisation=True,\n",
    "                    stop_words=None):\n",
    "\n",
    "        sentence = str(X)\n",
    "        sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "\n",
    "        tokens = word_tokenize(sentence)\n",
    "\n",
    "        if lowercase:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [word for word in tokens if word.isalnum()]\n",
    "\n",
    "        if removedigit:\n",
    "            tokens = [word for word in tokens if not word.isdigit()]\n",
    "\n",
    "        if removestopwords and stop_words is not None:\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        if getlemmatisation:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        if getstemmer:\n",
    "            ps = PorterStemmer()\n",
    "            tokens = [ps.stem(word) for word in tokens]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def preprocess_data(self, df, use_advanced_cleaning=False):\n",
    "        # Demojize\n",
    "        df['tweet_text'] = df['text'].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "        # Binary features\n",
    "        df['has_url'] = df['text'].str.contains(r'http[s]?://', regex=True)\n",
    "        df['has_mention'] = df['text'].str.contains(r'@\\w+', regex=True)\n",
    "        df['has_hashtag'] = df['text'].str.contains(r'#\\w+', regex=True)\n",
    "        df['has_emoji'] = df['tweet_text'].str.contains(r':[^:\\s]+:', regex=True)\n",
    "\n",
    "        # Clean text\n",
    "        if use_advanced_cleaning:\n",
    "            print(\"üßπ Nettoyage avanc√© + lemmatisation...\")\n",
    "            df['text_clean'] = df['tweet_text'].apply(self.clean_text_light)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            df['text_clean'] = df['text_clean'].apply(lambda x: self.MyCleanText(\n",
    "                x,\n",
    "                lowercase=True,\n",
    "                removestopwords=False,\n",
    "                removedigit=True,\n",
    "                getstemmer=False,\n",
    "                getlemmatisation=True,\n",
    "                stop_words=stop_words\n",
    "            ))\n",
    "        else:\n",
    "            print(\"üßΩ Nettoyage simple (light)...\")\n",
    "            df['text_clean'] = df['tweet_text'].apply(self.clean_text_light)\n",
    "\n",
    "        df.dropna(subset=['text_clean'], inplace=True)\n",
    "        return df, 'text_clean'\n",
    "\n",
    "    def add_text_features(self, df):\n",
    "        df['tweet_text'] = df['text'].apply(lambda x: emoji.demojize(x))\n",
    "        df['has_url'] = df['text'].str.contains(r'http[s]?://', regex=True)\n",
    "        df['has_mention'] = df['text'].str.contains(r'@\\w+', regex=True)\n",
    "        df['has_hashtag'] = df['text'].str.contains(r'#\\w+', regex=True)\n",
    "        df['has_emoji'] = df['tweet_text'].str.contains(r':[^:\\s]+:', regex=True)\n",
    "        return df\n",
    "\n",
    "    def vectorize_text(self, df, use_cleaned=False):\n",
    "        text_column = 'text_clean' if use_cleaned and 'text_clean' in df.columns else 'tweet_text'\n",
    "        df = df.dropna(subset=[text_column])\n",
    "        X_text = df[text_column]\n",
    "\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            lowercase=True,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            max_df=0.9,\n",
    "            min_df=2\n",
    "        )\n",
    "        X_tfidf = vectorizer.fit_transform(X_text)\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        extra_features = df[['has_url', 'has_mention', 'has_hashtag']].astype(int)\n",
    "        X_extra = scipy.sparse.csr_matrix(extra_features.values)\n",
    "\n",
    "        X_full = scipy.sparse.hstack([X_tfidf, X_extra])\n",
    "        print(\"üî¢ Taille TF-IDF :\", X_tfidf.shape)\n",
    "        print(\"‚ûï Taille features binaires :\", X_extra.shape)\n",
    "        print(\"üìê Taille finale :\", X_full.shape)\n",
    "\n",
    "        return X_tfidf, X_full, vectorizer\n",
    "\n",
    "\n",
    "    def extract_cleaned_features(self, X_tfidf, vectorizer):\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n",
    "\n",
    "        def get_token_category(token):\n",
    "            if token.isdigit():\n",
    "                return 'numeric'\n",
    "            if re.match(r'^\\d+(am|pm|s|h|min|sec|hour|years)?$', token.lower()):\n",
    "                return 'timestamp'\n",
    "            if re.match(r'^[a-zA-Z]+$', token):\n",
    "                return 'valid'\n",
    "            return 'other'\n",
    "\n",
    "        token_categories = {token: get_token_category(token) for token in tfidf_df.columns}\n",
    "        valid_tokens = [t for t, c in token_categories.items() if c == 'valid']\n",
    "        numeric_tokens = [t for t, c in token_categories.items() if c == 'numeric']\n",
    "        timestamp_tokens = [t for t, c in token_categories.items() if c == 'timestamp']\n",
    "\n",
    "        tfidf_df['numeric'] = tfidf_df[numeric_tokens].sum(axis=1) if numeric_tokens else 0\n",
    "        tfidf_df['timestamp'] = tfidf_df[timestamp_tokens].sum(axis=1) if timestamp_tokens else 0\n",
    "        tfidf_cleaned = tfidf_df[valid_tokens + ['numeric', 'timestamp']]\n",
    "        X_cleaned = scipy.sparse.csr_matrix(tfidf_cleaned.values)\n",
    "\n",
    "        print(\"üßº Matrice nettoy√©e :\", tfidf_cleaned.shape)\n",
    "        return X_cleaned, tfidf_cleaned\n",
    "\n",
    "    def evaluate_models(self, X, y):\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        from sklearn.svm import LinearSVC\n",
    "\n",
    "        models = {\n",
    "            \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "            \"Multinomial NB\": MultinomialNB(),\n",
    "            \"Random Forest\": RandomForestClassifier(),\n",
    "            \"SVM\": SVC(),\n",
    "            \"SVM linear\": LinearSVC(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"AdaBoost\": AdaBoostClassifier(),\n",
    "            \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "            \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "            \"Neural Network (MLP)\": MLPClassifier(max_iter=300)\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        results = []\n",
    "\n",
    "        scoring_metrics = {\n",
    "            'Accuracy': make_scorer(accuracy_score),\n",
    "            'Precision': make_scorer(precision_score),\n",
    "            'Recall': make_scorer(recall_score),\n",
    "            'F1': make_scorer(f1_score)\n",
    "        }\n",
    "\n",
    "        for name, model in models.items():\n",
    "            row = {'Model': name}\n",
    "            for metric_name, scorer in scoring_metrics.items():\n",
    "                scores = cross_val_score(model, X, y, cv=skf, scoring=scorer)\n",
    "                row[metric_name] = f\"{scores.mean():.3f} ¬± {scores.std():.3f}\"\n",
    "            results.append(row)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df.sort_values(by=\"F1\", ascending=False)\n",
    "        print(\"üìä R√©sultats de la validation crois√©e (tri√©s par F1) :\")\n",
    "        display(results_df)\n",
    "        return results_df\n",
    "\n",
    "\n",
    "    def save_best_model(self):\n",
    "        \"\"\"Save the best performing model\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"No best model found. Please run train_and_evaluate first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nSaving best model: {self.best_model['name']}\")\n",
    "        joblib.dump(self.best_model, 'best_model_task1.joblib')\n",
    "        print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702289d3-6e2f-4372-bfc4-a0eb347d4729",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6a19f-ce14-421b-9842-3ad6d1cbc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('scitweets_export_tache2_60_40.tsv', sep='\\t')\n",
    "\n",
    "# Display sample of the data\n",
    "print(\"\\nSample of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ModelPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d543e48-50ee-4784-aafc-1d242c1cda43",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beee2df-1417-4834-836a-52d8816ea495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sci = pipeline.analyze_data(df)\n",
    "\n",
    "df_sci = pipeline.create_label(df_sci)\n",
    "\n",
    "df_sci, text_column = pipeline.preprocess_data(df_sci, use_advanced_cleaning=False)\n",
    "\n",
    "X_tfidf, X_full, vect = pipeline.vectorize_text(df_sci, use_cleaned=False)\n",
    "\n",
    "X, _ = pipeline.extract_cleaned_features(X_tfidf, vect)\n",
    "\n",
    "y = df_sci[\"label\"]\n",
    "results_df = pipeline.evaluate_models(X, y)\n",
    "\n",
    "#pipeline.save_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe26d3-8097-4183-96d1-049e067fc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sci = pipeline.analyze_data(df)\n",
    "\n",
    "df_sci = pipeline.create_label(df_sci)\n",
    "\n",
    "df_sci, text_column = pipeline.preprocess_data(df_sci, use_advanced_cleaning=True)\n",
    "\n",
    "X_tfidf, X_full, vect = pipeline.vectorize_text(df_sci, use_cleaned=True)\n",
    "\n",
    "X_cleaned, _ = pipeline.extract_cleaned_features(X_tfidf, vect)\n",
    "\n",
    "y = df_sci[\"label\"]\n",
    "results_df = pipeline.evaluate_models(X_cleaned, y)\n",
    "\n",
    "pipeline.save_best_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9470a1-d45a-4793-9344-4447b6dce211",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0569be2-2029-42ef-8ab1-357e256d65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "\n",
    "class DataVisualizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reduce_dimensions(self, X, method=\"svd\"):\n",
    "        if method == \"svd\":\n",
    "            reducer = TruncatedSVD(n_components=2, random_state=42)\n",
    "        else:\n",
    "            reducer = PCA(n_components=2, random_state=42)\n",
    "\n",
    "        return reducer.fit_transform(X)\n",
    "\n",
    "    def plot_decision_boundary(self, model, X_2D, y, title):\n",
    "        h = .02\n",
    "        x_min, x_max = X_2D[:, 0].min() - 1, X_2D[:, 0].max() + 1\n",
    "        y_min, y_max = X_2D[:, 1].min() - 1, X_2D[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        model.fit(X_2D, y)\n",
    "        try:\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur: {e}\")\n",
    "            return\n",
    "\n",
    "        cmap_light = ListedColormap(['#FFCCCC', '#CCCCFF'])\n",
    "        cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.3)\n",
    "        plt.scatter(X_2D[:, 0], X_2D[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "        plt.title(f\"Fronti√®re de d√©cision : {title}\")\n",
    "        plt.xlabel(\"Composante 1\")\n",
    "        plt.ylabel(\"Composante 2\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_multiple_boundaries(self, X, y):\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        X_vis = pca.fit_transform(X.toarray())\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_vis)\n",
    "\n",
    "        classifiers = {\n",
    "            \"Logistic Regression\": LogisticRegression(),\n",
    "            \"Gaussian NB\": GaussianNB(),\n",
    "            \"Decision Tree\": DecisionTreeClassifier(),\n",
    "            \"SVM\": SVC()\n",
    "        }\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.ravel()\n",
    "        colors = ['blue', 'orange']\n",
    "        markers = ['o', 's']\n",
    "\n",
    "        for idx, (name, clf) in enumerate(classifiers.items()):\n",
    "            clf.fit(X_scaled, y)\n",
    "            accuracy = clf.score(X_scaled, y)\n",
    "\n",
    "            DecisionBoundaryDisplay.from_estimator(\n",
    "                clf, X_scaled, cmap=plt.cm.Paired, response_method=\"predict\", alpha=0.8, ax=axes[idx]\n",
    "            )\n",
    "\n",
    "            for label, marker, color in zip([0, 1], markers, colors):\n",
    "                axes[idx].scatter(X_scaled[y == label][:, 0], X_scaled[y == label][:, 1],\n",
    "                                  c=color, marker=marker, edgecolor='k', label=f\"Classe {label}\" if idx == 0 else \"\")\n",
    "\n",
    "            axes[idx].set_title(f\"{name} (Accuracy: {accuracy:.2f})\", fontsize=12)\n",
    "            axes[idx].set_xlabel(\"PC1\")\n",
    "            axes[idx].set_ylabel(\"PC2\")\n",
    "\n",
    "        handles = [plt.Line2D([0], [0], marker=markers[i], color='w', markerfacecolor=colors[i], markeredgecolor='k', markersize=10, label=f\"Classe {i}\") for i in range(2)]\n",
    "        fig.legend(handles=handles, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_roc_curve(self, y_true, y_scores, model_name=\"Model\"):\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, marker='o', linestyle='-', color='blue', label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random classifier')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate (FPR)')\n",
    "        plt.ylabel('True Positive Rate (TPR)')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_dimensionality_reduction(self, X, y):\n",
    "        df_y = pd.Series(y).astype(str)\n",
    "        X_dense = X.toarray()\n",
    "\n",
    "        # PCA 2D\n",
    "        pca = PCA(n_components=2, random_state=0)\n",
    "        components_2d = pca.fit_transform(X_dense)\n",
    "        fig_pca_2d = px.scatter(pd.DataFrame(components_2d), x=0, y=1, color=df_y, labels={\"color\": \"Label\"})\n",
    "        fig_pca_2d.update_layout(title='ACP (2D)')\n",
    "        fig_pca_2d.show()\n",
    "\n",
    "        # PCA 3D\n",
    "        pca3 = PCA(n_components=3, random_state=0)\n",
    "        components_3d = pca3.fit_transform(X_dense)\n",
    "        fig_pca_3d = px.scatter_3d(pd.DataFrame(components_3d), x=0, y=1, z=2, color=df_y, title='ACP (3D)',\n",
    "                                   labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'})\n",
    "        fig_pca_3d.show()\n",
    "\n",
    "        # TSNE 2D\n",
    "        tsne2d = TSNE(n_components=2, random_state=0)\n",
    "        tsne_proj_2d = tsne2d.fit_transform(X_dense)\n",
    "        fig_tsne_2d = px.scatter(pd.DataFrame(tsne_proj_2d), x=0, y=1, color=df_y, labels={'color': 'Label'})\n",
    "        fig_tsne_2d.update_layout(title='t-SNE (2D)')\n",
    "        fig_tsne_2d.show()\n",
    "\n",
    "        # TSNE 3D\n",
    "        tsne3d = TSNE(n_components=3, random_state=0)\n",
    "        tsne_proj_3d = tsne3d.fit_transform(X_dense)\n",
    "        fig_tsne_3d = px.scatter_3d(pd.DataFrame(tsne_proj_3d), x=0, y=1, z=2, color=df_y, labels={'color': 'Label'})\n",
    "        fig_tsne_3d.update_layout(title='t-SNE (3D)')\n",
    "        fig_tsne_3d.show()\n",
    "\n",
    "        # UMAP 2D\n",
    "        umap2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "        umap_proj_2d = umap2d.fit_transform(X_dense)\n",
    "        fig_umap_2d = px.scatter(pd.DataFrame(umap_proj_2d), x=0, y=1, color=df_y, labels={'color': 'Label'})\n",
    "        fig_umap_2d.update_layout(title='UMAP (2D)')\n",
    "        fig_umap_2d.show()\n",
    "\n",
    "        # UMAP 3D\n",
    "        umap3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "        umap_proj_3d = umap3d.fit_transform(X_dense)\n",
    "        fig_umap_3d = px.scatter_3d(pd.DataFrame(umap_proj_3d), x=0, y=1, z=2, color=df_y, labels={'color': 'Label'})\n",
    "        fig_umap_3d.update_layout(title='UMAP (3D)')\n",
    "        fig_umap_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120e3f9-fb54-4cdf-a7a8-a0bea8f19fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation apr√®s vectorisation\n",
    "visualizer = DataVisualizer()\n",
    "\n",
    "# R√©duction SVD en 2D (sur X_final ou X_cleaned) pour fronti√®re de d√©cision simple\n",
    "X_2D = visualizer.reduce_dimensions(X_cleaned, method=\"svd\")  # ou \"pca\"\n",
    "\n",
    "# Visualisation des fronti√®res, tracer la fronti√®re d‚Äôun mod√®le simple\n",
    "visualizer.plot_decision_boundary(LogisticRegression(), X_2D, y, \"Logistic Regression\")\n",
    "\n",
    "# Plusieurs mod√®les d‚Äôun coup, Tracer 2x2 plusieurs mod√®les avec PCA\n",
    "visualizer.plot_multiple_boundaries(X_cleaned, y)\n",
    "\n",
    "# Courbes interactives PCA/tSNE/UMAP en 2D/3D\n",
    "visualizer.plot_dimensionality_reduction(X_cleaned, y)\n",
    "\n",
    "\n",
    "# (optionnel) ROC si on a y_scores (sortie de .predict_proba ou .decision_function)\n",
    "model = MLPClassifier(max_iter=300, random_state=42)\n",
    "y_scores = cross_val_predict(model, X, y, cv=skf, method='predict_proba')[:, 1]\n",
    "visualizer.plot_roc_curve(y, y_scores, model_name=\"MLP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16e245-26f0-48ec-9f56-4148864ab9aa",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300883c-92e8-4729-af4f-9a21741f42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "\n",
    "for i, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(1, 4, i)\n",
    "    sns.barplot(data=results, x='Model', y=metric)\n",
    "    plt.title(f'{metric} Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c1786-9a10-4fd6-8a38-41b1b78e1ec1",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d08a25-855d-47ee-af42-7f2e1442d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self):\n",
    "        self.best_models = {}\n",
    "\n",
    "    def tune_and_evaluate(self, model_name, model, param_grid, X, y):\n",
    "        print(f\"\\nüîç Recherche d'hyperparam√®tres pour {model_name}...\")\n",
    "        search = GridSearchCV(model, param_grid=param_grid, scoring='f1', cv=5)\n",
    "        search.fit(X, y)\n",
    "\n",
    "        print(f\"‚úÖ Meilleurs param√®tres {model_name} :\", search.best_params_)\n",
    "\n",
    "        y_pred = cross_val_predict(search.best_estimator_, X, y, cv=5)\n",
    "\n",
    "        print(f\"\\nüìä Rapport de classification - {model_name} (validation crois√©e) :\")\n",
    "        print(classification_report(y, y_pred))\n",
    "\n",
    "        self.best_models[model_name] = search.best_estimator_\n",
    "\n",
    "    def tune_all(self, X, y):\n",
    "        param_svm = {\n",
    "            'C': [0.01, 0.1, 1, 10]\n",
    "        }\n",
    "        self.tune_and_evaluate(\"SVM (lin√©aire)\", LinearSVC(), param_svm, X, y)\n",
    "\n",
    "        param_mlp = {\n",
    "            'hidden_layer_sizes': [(100,), (100,50)],\n",
    "            'alpha': [0.0001, 0.001],\n",
    "            'learning_rate_init': [0.001, 0.01]\n",
    "        }\n",
    "        self.tune_and_evaluate(\"MLP\", MLPClassifier(max_iter=300, random_state=42), param_mlp, X, y)\n",
    "\n",
    "        param_nb = {\n",
    "            'alpha': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "        self.tune_and_evaluate(\"Multinomial NB\", MultinomialNB(), param_nb, X, y)\n",
    "\n",
    "    def get_best_model(self, name):\n",
    "        return self.best_models.get(name, None)\n",
    "\n",
    "    def evaluate_model_cv(self, model, X, y, cv=10):\n",
    "        accs, precs, recalls, f1s = [], [], [], []\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            accs.append(accuracy_score(y_test, y_pred))\n",
    "            precs.append(precision_score(y_test, y_pred))\n",
    "            recalls.append(recall_score(y_test, y_pred))\n",
    "            f1s.append(f1_score(y_test, y_pred))\n",
    "\n",
    "        print(\"\\nüî¨ R√©sultats avec CV (moyenne ¬± √©cart-type):\")\n",
    "        print(f\"Accuracy : {np.mean(accs):.3f} ¬± {np.std(accs):.3f}\")\n",
    "        print(f\"Precision: {np.mean(precs):.3f} ¬± {np.std(precs):.3f}\")\n",
    "        print(f\"Recall   : {np.mean(recalls):.3f} ¬± {np.std(recalls):.3f}\")\n",
    "        print(f\"F1-score : {np.mean(f1s):.3f} ¬± {np.std(f1s):.3f}\")\n",
    "\n",
    "    def run_optuna(self, X, y, model_choisi):\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        def objective_svm(trial):\n",
    "            C = trial.suggest_float(\"C\", 1e-3, 10, log=True)\n",
    "            clf = LinearSVC(C=C, max_iter=1000)\n",
    "            scores = cross_val_score(clf, X, y, cv=skf, scoring='f1')\n",
    "            return scores.mean()\n",
    "\n",
    "        def objective_svc(trial):\n",
    "            kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "            params = {\n",
    "                'C': trial.suggest_float('C', 1e-2, 10, log=True),\n",
    "                'kernel': kernel,\n",
    "                'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "            }\n",
    "            if kernel == 'poly':\n",
    "                params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "            model = SVC(**params)\n",
    "            scores = cross_val_score(model, X, y, cv=skf, scoring='f1', n_jobs=-1)\n",
    "            return scores.mean()\n",
    "\n",
    "        def objective_mlp(trial):\n",
    "            hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\", [(100,), (100, 50), (150,)])\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-2, log=True)\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 1e-4, 1e-1, log=True)\n",
    "            clf = make_pipeline(\n",
    "                StandardScaler(with_mean=False),\n",
    "                MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, alpha=alpha,\n",
    "                             learning_rate_init=learning_rate_init, max_iter=300, random_state=42)\n",
    "            )\n",
    "            scores = cross_val_score(clf, X, y, cv=skf, scoring='f1')\n",
    "            return scores.mean()\n",
    "\n",
    "        def objective_mlp2(trial):\n",
    "            hidden_layer_sizes = (\n",
    "                trial.suggest_int('hidden_layer_1', 50, 200),\n",
    "                trial.suggest_int('hidden_layer_2', 0, 150)\n",
    "            )\n",
    "            hidden_layer_sizes = tuple([h for h in hidden_layer_sizes if h > 0])\n",
    "            clf = make_pipeline(\n",
    "                StandardScaler(with_mean=False),\n",
    "                MLPClassifier(\n",
    "                    hidden_layer_sizes=hidden_layer_sizes,\n",
    "                    learning_rate_init=trial.suggest_float('learning_rate_init', 0.0001, 0.1, log=True),\n",
    "                    activation=trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "                    alpha=trial.suggest_float('alpha', 1e-5, 1e-1, log=True),\n",
    "                    max_iter=300, random_state=42\n",
    "                )\n",
    "            )\n",
    "            scores = cross_val_score(clf, X, y, cv=skf, scoring='f1', n_jobs=-1)\n",
    "            return scores.mean()\n",
    "\n",
    "        def objective_nb(trial):\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-3, 2.0, log=True)\n",
    "            clf = MultinomialNB(alpha=alpha)\n",
    "            scores = cross_val_score(clf, X, y, cv=skf, scoring='f1')\n",
    "            return scores.mean()\n",
    "        \n",
    "        if model_choisi==\"svm\" :\n",
    "            \n",
    "            # SVM\n",
    "            study_svm = optuna.create_study(direction=\"maximize\")\n",
    "            study_svm.optimize(objective_svm, n_trials=200)\n",
    "            print(\"\\nüîç Best params SVM:\", study_svm.best_params)\n",
    "            print(\"ü•á Best F1 score SVM:\", study_svm.best_value)\n",
    "            best_svm = LinearSVC(C=study_svm.best_params[\"C\"], random_state=42)\n",
    "            self.evaluate_model_cv(best_svm, X, y)\n",
    "            vis.plot_optimization_history(study_svm).show()\n",
    "            vis.plot_param_importances(study_svm).show()\n",
    "\n",
    "        if model_choisi==\"svc\" :\n",
    "\n",
    "            # SVC\n",
    "            study_svc = optuna.create_study(direction=\"maximize\")\n",
    "            study_svc.optimize(objective_svc, n_trials=200)\n",
    "            params = study_svc.best_params.copy()\n",
    "            if params['kernel'] != 'poly' and 'degree' in params:\n",
    "                del params['degree']\n",
    "            best_svc = SVC(**params)\n",
    "            print(\"\\nüîç Best SVC params:\", params)\n",
    "            print(\"ü•á Best F1 score SVC:\", study_svc.best_value)\n",
    "            self.evaluate_model_cv(best_svc, X, y)\n",
    "            vis.plot_optimization_history(study_svc).show()\n",
    "            vis.plot_param_importances(study_svc).show()\n",
    "\n",
    "        if model_choisi==\"mlp1\" :\n",
    "\n",
    "            # MLP (v1)\n",
    "            study_mlp = optuna.create_study(direction=\"maximize\")\n",
    "            study_mlp.optimize(objective_mlp, n_trials=50)\n",
    "            print(\"\\nüîç Best MLP params:\", study_mlp.best_params)\n",
    "            print(\"ü•á Best F1 score MLP:\", study_mlp.best_value)\n",
    "            best_mlp = make_pipeline(\n",
    "                StandardScaler(with_mean=False),\n",
    "                MLPClassifier(**study_mlp.best_params, max_iter=300, random_state=42)\n",
    "            )\n",
    "            self.evaluate_model_cv(best_mlp, X, y)\n",
    "            vis.plot_optimization_history(study_mlp).show()\n",
    "            vis.plot_param_importances(study_mlp).show()\n",
    "\n",
    "        if model_choisi==\"mlp2\" :\n",
    "\n",
    "            # MLP (v2)\n",
    "            study_mlp2 = optuna.create_study(direction=\"maximize\")\n",
    "            study_mlp2.optimize(objective_mlp2, n_trials=50)\n",
    "            params = study_mlp2.best_params\n",
    "            hidden_layer_sizes = []\n",
    "            if 'hidden_layer_1' in params:\n",
    "                hidden_layer_sizes.append(params['hidden_layer_1'])\n",
    "            if 'hidden_layer_2' in params and params['hidden_layer_2'] > 0:\n",
    "                hidden_layer_sizes.append(params['hidden_layer_2'])\n",
    "            best_mlp2 = make_pipeline(\n",
    "                StandardScaler(with_mean=False),\n",
    "                MLPClassifier(\n",
    "                    hidden_layer_sizes=tuple(hidden_layer_sizes),\n",
    "                    learning_rate_init=params['learning_rate_init'],\n",
    "                    activation=params['activation'],\n",
    "                    alpha=params['alpha'],\n",
    "                    max_iter=300,\n",
    "                    random_state=42\n",
    "                )\n",
    "            )\n",
    "            print(\"\\nüîç Study 2 - Best MLP params:\", params)\n",
    "            print(\"ü•á Study 2 - Best F1 score MLP:\", study_mlp2.best_value)\n",
    "            self.evaluate_model_cv(best_mlp2, X, y)\n",
    "            vis.plot_optimization_history(study_mlp2).show()\n",
    "            vis.plot_param_importances(study_mlp2).show()\n",
    "\n",
    "        if model_choisi==\"mb\" :\n",
    "            # MultinomialNB\n",
    "            study_nb = optuna.create_study(direction=\"maximize\")\n",
    "            study_nb.optimize(objective_nb, n_trials=200)\n",
    "            print(\"\\nüîç Best MultinomialNB params:\", study_nb.best_params)\n",
    "            print(\"ü•á Best F1 score MultinomialNB:\", study_nb.best_value)\n",
    "            best_nb = MultinomialNB(**study_nb.best_params)\n",
    "            self.evaluate_model_cv(best_nb, X, y)\n",
    "            vis.plot_optimization_history(study_nb).show()\n",
    "            vis.plot_param_importances(study_nb).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec99a7-15b4-4471-8e07-fcb1a22f5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tuner = HyperparameterTuner()\n",
    "#tuner.tune_all(X_cleaned, y) \n",
    "\n",
    "# R√©cup√©rer un mod√®le pour encha√Æner ensuite\n",
    "#best_mlp = tuner.get_best_model(\"MLP\")\n",
    "\n",
    "tuner.run_optuna(X_cleaned, y, \"mlp1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fee7d-48a8-4335-a073-9abff2f5c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.run_optuna(X_cleaned, y, \"mlp2\")  # ou X_cleaned selon ta vectorisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca86d26-bec5-4bb0-902a-6022ee5d92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner()\n",
    "\n",
    "tuner.run_optuna(X_cleaned, y, \"mb\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b16dc2-e28c-4d79-b71b-3a7cad4221d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
